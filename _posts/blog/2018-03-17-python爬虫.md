---
layout:     post
title: python爬虫
description: 作者没有大型python经验，可能写的有点烂
category: blog
---

## 一个bing的爬虫 ##
作用仅限于下载一张首页的背景图。拿来作作windows桌面效果还不错。

<pre>

from urllib import request
import re

bingeverydayimgprefix  = 'https://www.bing.com/az/hprichbg/rb/'

with request.urlopen('https://www.bing.com/') as f:
    data = f.read()
    #print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        pass
    bingdata = data.decode('utf-8')

def findimg(htmlstring):
    return re.findall(r'\.jpg', htmlstring)

print("===================")
bracket = re.findall(r'\{.*?\}', bingdata)
cnt = 0
jpglist=[]
for i in bracket:
    if findimg(i):
        cnt += 1
        jpglist.append(i)

print("here has %d jpg element(s)" % cnt)
truejpegurl=[]
jpegfilename=[]
for i in jpglist:
    result = [x for x in re.split("/|\'|\"", i) if x]
    for j in result:
        if findimg(j):
            truejpegurl.append(bingeverydayimgprefix + j)
            jpegfilename.append(j)
print(truejpegurl)
print(jpegfilename)

print("===========================================================")
import urllib.request

def getImg(url,filename):
    urllib.request.urlretrieve(url, filename)

n=0
for cururl in truejpegurl:
    getImg(truejpegurl[n], jpegfilename[n])
    n += 1

</pre>








## 一个http请求的发送 ##
bing到底和作者什么仇？
<pre>
import socket

# 创建socket:
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# 连接:

s.connect(('www.bing.com', 80))

request = 'GET / HTTP/1.1 \r\n\
Host: cn.bing.com\r\n\
User-Agent:Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\r\n\
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\r\n\
Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\r\n\
Connection: close\r\n\
Upgrade-Insecure-Requests: 1\r\n\
Pragma: no-cache\r\n\
Cache-Control: no-cache\r\n\r\n'

s.send(bytes(request, encoding = "utf8"))

# 接收:
buffer = []
while True:
    d = s.recv(1024)
    if d:
        buffer.append(d)
    else:
        break
data = b''.join(buffer)

s.close()

header, html = data.split(b'\r\n\r\n', 1)
print(header.decode())
with open('bing.html', 'wb') as f:
    f.write(html)
</pre>










## 爬爬漫画 ##
实测效果不是很好。跑了一下午一部《死神》都没爬完。还需要继续培养面向对象的编程思想。而且总是用正则，代码不简洁；应该有第三方模块可以直接import后处理获取到的一些数据
只实现了imanhua的一小部分，manhuatai还没有实现。

<pre>
#coding=utf-8
from urllib import request
import re
import base64
import os
from selenium import webdriver
#browser = webdriver.PhantomJS()

class manhuatai(object):      ####todo
    def __init__(self, url):
        self.url = url
    def introduce(self):
        with request.urlopen(self.url) as f:
            data = f.read()
            # print('Status:', f.status, f.reason)
            for k, v in f.getheaders():
                pass
            self.urldata = data.decode('utf-8')                                                                 #manhuatai用了utf-8编码，因此decode参数为utf-8
            self.urldatalist = []
            bracket=re.findall(r'\<.*?\>', self.urldata)
            for i in bracket:
                self.urldatalist.append(i)
                #print(i)
    def setepisode2dict(self):
        self.episodedict = {}
        for i in self.urldatalist:
            m = re.match(r'^.*?title\=\"(.*?)\"\s.*?\"(.*\.html)\".*$', i)
            if m:
                self.episodedict[m.group(1)]=m.group(2)
                #print("{}==={}".format(m.group(1),m.group(2)))


    def requestepisode(self):
        print(self.url + '/' + self.episodedict['第01期'])
        with request.urlopen(self.url + '/' + self.episodedict['第01期']) as f:
            data = f.read()
            # print('Status:', f.status, f.reason)
            #for k, v in f.getheaders():
            #    pass
            episodedata = data.decode('utf-8')
            print(episodedata)                                                                                  #找不到每一张图片的url请求，先搁置
        browser.get(self.url + '/' + self.episodedict['第01期'])                                                #用webdriver的方法去请求，实际PhantomJS是一个无界面的浏览器内核
        print("need open line 6")
        data = browser.page_source                                                                              #页面源码
        #print(data)

    def getImg(self, url,filename):
        request.urlretrieve(url, filename)

class iimanhua(manhuatai):
    def introduce(self):
        with request.urlopen(self.url) as f:                                                                    #请求介绍页，获取漫画列表
            data = f.read()
            # print('Status:', f.status, f.reason)
            for k, v in f.getheaders():
                #print("{}={}".format(k, v))
                pass
            #print(data.decode('gb2312', errors='ignore'))                                                      #imanhua网站用了gb2312编码，因此decode参数为gb2312
            self.urldata = data.decode('gb2312', errors='ignore')
            self.urldatalist = []
            bracket=re.findall(r'\<.*?\>', self.urldata)
            for i in bracket:
                self.urldatalist.append(i)
                #print(i)
    def getImg(self, url,filename):
        request.urlretrieve(url, filename)
    def setepisode2dict(self):                                                                                  #将每一话的相对路径和标题存入self.episodedict
        self.episodedict = {}
        for i in self.urldatalist:
            m = re.match(r'^.*?\"(\/imanhua.*?\.html)\"\s+title=\"(.*?)\".*?target.*$', i)
            if m:
                self.episodedict[m.group(2)]=m.group(1)
                #print("{}={}".format(m.group(1),m.group(2)))

        for k, v in self.episodedict.items():
            pass
            #print("{}={}".format(k, v))    #第x话='xxx.html'
    def requestepisode(self):                                                                                   #处理每一话
        self.mainurl='http://www.iimanhua.com/'
        for k, v in self.episodedict.items():
            print("{}={}".format(k,v))
            with request.urlopen(self.mainurl + '/' + v) as f:                                                          #打开每一话的第一页，每张图片是异步加载的。但是页面源码可以获取base64编码的图片url
            #with request.urlopen('http://www.iimanhua.com/imanhua/9202/261288.html') as f:
                data = f.read()
                self.episode = data.decode('gb2312', errors='ignore')
                m = re.findall(r'var\sqTcms_S_m_murl_e=\"(.*?)\";', self.episode)
                #print(self.episode)
                allpicurl = str(base64.urlsafe_b64decode(m[0]), encoding = "utf-8").split('$qingtiandy$')               #将var qTcms_S_m_murl_e='xxx'的内容进行base64解码，并通过$qingtiandy$分割
                print(allpicurl)                                                                                            #这就是实际图片的url的一个list
                cnt=1
                if os.path.exists(k) != True:
                    os.mkdir('{}\\{}'.format('sishen', k))                                                              #创建一个文件夹存储下载的图片数据.现在只支持死神。若要添加其他的，需要修改此处以及下方sishen以及class的url
                for i in allpicurl:
                    try:
                        self.getImg('http://www.iimanhua.com/' + i, "{}\\{}\\{}.jpg".format('sishen', k, cnt))      #实际下载。部分图片url是相对地址，需要加前缀
                    except BaseException:
                        try:
                            self.getImg(i, "{}\\{}\\{}.jpg".format('sishen', k, cnt))                                   #部分url是绝对地址，不需要加前缀
                        except BaseException:
                            print("something err, ignore")                                                              #图片会下载失败，可能是服务器缘故，先这样处理
                            break
                    cnt+=1





comic=iimanhua("http://www.iimanhua.com/imanhua/sishen/")
comic.introduce()
comic.setepisode2dict()
comic.requestepisode()
#comic.getImg("http://cartoon.youzu88.com/manhuatuku/106/manhua_12_20160717_2016071711302992378.jpg", "1.jpg")


#browser.close()
</pre>